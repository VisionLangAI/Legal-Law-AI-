{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfr0sAUDu-K7"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# LexFaith-HierBERT â€” One-File Reference Code\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------------------\n",
        "# Config (edit here)\n",
        "# -------------------\n",
        "CFG = dict(\n",
        "    seed=13,\n",
        "    # sequence/segments\n",
        "    max_seq_len=512,\n",
        "    segment_len=512,\n",
        "    segment_stride=64,\n",
        "    max_segments=12,\n",
        "    # training\n",
        "    batch_size=4,\n",
        "    epochs_A=3,            # keep small for demo\n",
        "    epochs_B=3,\n",
        "    lr_encoder=2e-5,\n",
        "    lr_head=5e-5,\n",
        "    dropout=0.1,\n",
        "    early_stop_patience=2,\n",
        "    # multi-label classes (Task B)\n",
        "    num_labels=10,\n",
        "    # flags\n",
        "    use_transformers=True  # set False to run BiLSTM baseline only\n",
        ")\n",
        "\n",
        "# ==============\n",
        "# Reproducibility\n",
        "# ==============\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "try:\n",
        "    import torch\n",
        "    def _torch_seed(s):\n",
        "        torch.manual_seed(s);\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(s)\n",
        "    set_seed(CFG[\"seed\"]); _torch_seed(CFG[\"seed\"])\n",
        "except Exception:\n",
        "    set_seed(CFG[\"seed\"])\n",
        "\n",
        "# ==========================\n",
        "# Tokenizer (flexible import)\n",
        "# ==========================\n",
        "class FallbackWS:\n",
        "    def __init__(self):\n",
        "        # fake \"ids\": small vocab\n",
        "        self.cls_token_id = 101\n",
        "        self.sep_token_id = 102\n",
        "        self.pad_token_id = 0\n",
        "        self.vocab = {}\n",
        "        self._next = 200\n",
        "    def __call__(self, text, add_special_tokens=False, padding=False, truncation=False,\n",
        "                 max_length=None, return_tensors=None):\n",
        "        toks = text.split()\n",
        "        ids = []\n",
        "        for t in toks:\n",
        "            if t not in self.vocab:\n",
        "                self.vocab[t] = self._next; self._next += 1\n",
        "            ids.append(self.vocab[t])\n",
        "        if truncation and max_length:\n",
        "            ids = ids[:max_length]\n",
        "        return {\"input_ids\": ids}\n",
        "    def encode_plus(self, text, **kw): return self(text, **kw)\n",
        "\n",
        "def build_tokenizer(name=\"bert-base-uncased\", use_transformers=True):\n",
        "    if use_transformers:\n",
        "        try:\n",
        "            from transformers import AutoTokenizer\n",
        "            return AutoTokenizer.from_pretrained(name, use_fast=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return FallbackWS()\n",
        "\n",
        "TOK = build_tokenizer(use_transformers=CFG[\"use_transformers\"])\n",
        "\n",
        "# =================\n",
        "# Segment utilities\n",
        "# =================\n",
        "def segment_text_ids(ids, segment_len=512, stride=64, cls_id=101, sep_id=102):\n",
        "    segs = []\n",
        "    i = 0\n",
        "    K = max(1, segment_len-2)\n",
        "    while i < len(ids):\n",
        "        chunk = ids[i:i+K]\n",
        "        seg = [cls_id] + chunk + [sep_id]\n",
        "        segs.append(seg[:segment_len])\n",
        "        if i+K >= len(ids): break\n",
        "        i += (segment_len - stride)\n",
        "    return segs\n",
        "\n",
        "def text_to_segments(text, tokenizer, cfg):\n",
        "    out = tokenizer(text, add_special_tokens=False)\n",
        "    ids = out[\"input_ids\"]\n",
        "    return segment_text_ids(ids, cfg[\"segment_len\"], cfg[\"segment_stride\"],\n",
        "                            getattr(tokenizer, \"cls_token_id\", 101),\n",
        "                            getattr(tokenizer, \"sep_token_id\", 102))\n",
        "\n",
        "def pad_segments(segs, max_segments, pad_id, seg_len):\n",
        "    segs = segs[:max_segments]\n",
        "    segs = [s + [pad_id]*(seg_len-len(s)) for s in segs]\n",
        "    while len(segs) < max_segments:\n",
        "        segs.append([pad_id]*seg_len)\n",
        "    return np.asarray(segs, dtype=np.int64)\n",
        "\n",
        "# =========\n",
        "# Metrics\n",
        "# =========\n",
        "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
        "                             precision_recall_fscore_support, hamming_loss)\n",
        "\n",
        "def metrics_taskA(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    acc  = float(accuracy_score(y_true, y_pred))\n",
        "    P,R,F1,_ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    try: auc = float(roc_auc_score(y_true, y_prob))\n",
        "    except Exception: auc = float(\"nan\")\n",
        "    return dict(accuracy=acc, precision=float(P), recall=float(R), f1=float(F1), roc_auc=auc)\n",
        "\n",
        "def metrics_taskB(Y_true, Y_prob, thr=0.5):\n",
        "    Y_pred = (Y_prob >= thr).astype(int)\n",
        "    micro_f1 = float(f1_score(Y_true, Y_pred, average=\"micro\", zero_division=0))\n",
        "    macro_f1 = float(f1_score(Y_true, Y_pred, average=\"macro\", zero_division=0))\n",
        "    h_loss   = float(hamming_loss(Y_true, Y_pred))\n",
        "    return dict(micro_f1=micro_f1, macro_f1=macro_f1, hamming_loss=h_loss)\n",
        "\n",
        "# ======================\n",
        "# Baselines (brief forms)\n",
        "# ======================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def run_bow_logreg_taskA(train_df, val_df):\n",
        "    pipe = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=3, max_features=60000)),\n",
        "        (\"clf\", LogisticRegression(max_iter=300))\n",
        "    ])\n",
        "    pipe.fit(train_df[\"text\"], train_df[\"label\"])\n",
        "    prob = pipe.predict_proba(val_df[\"text\"])[:,1]\n",
        "    return metrics_taskA(val_df[\"label\"].values, prob)\n",
        "\n",
        "# ===========================\n",
        "# Torch models (if available)\n",
        "# ===========================\n",
        "HAS_TORCH = True\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "except Exception:\n",
        "    HAS_TORCH = False\n",
        "\n",
        "class BiLSTMAttn(nn.Module):\n",
        "    def __init__(self, vocab_size=40000, emb_dim=128, hidden=128, num_classes=1, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n",
        "        self.attn = nn.Linear(2*hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc   = nn.Linear(2*hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        E, _ = self.lstm(self.emb(x))\n",
        "        a = torch.softmax(self.attn(E).squeeze(-1), dim=-1)            # [B,T]\n",
        "        ctx = torch.einsum(\"bt,btd->bd\", a, E)                         # [B,2H]\n",
        "        logits = self.fc(self.drop(ctx))\n",
        "        return logits, a\n",
        "\n",
        "# Transformers models (flat) + Hierarchical\n",
        "TRANS_OK = False\n",
        "if CFG[\"use_transformers\"] and HAS_TORCH:\n",
        "    try:\n",
        "        from transformers import AutoModel\n",
        "        TRANS_OK = True\n",
        "    except Exception:\n",
        "        TRANS_OK = False\n",
        "\n",
        "class LegalBERTFlat(nn.Module):\n",
        "    def __init__(self, name=\"bert-base-uncased\", num_classes=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(name)\n",
        "        d = self.encoder.config.hidden_size\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc   = nn.Linear(d, num_classes)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        o = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = o.last_hidden_state[:,0,:]\n",
        "        return self.fc(self.drop(cls))\n",
        "\n",
        "class LongformerFlat(nn.Module):\n",
        "    def __init__(self, name=\"allenai/longformer-base-4096\", num_classes=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(name)\n",
        "        d = self.encoder.config.hidden_size\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc   = nn.Linear(d, num_classes)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        o = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = o.last_hidden_state[:,0,:]\n",
        "        return self.fc(self.drop(cls))\n",
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w = nn.Linear(dim, dim)\n",
        "        self.v = nn.Linear(dim, 1, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, H):                         # [B,S,D]\n",
        "        s = self.v(torch.tanh(self.w(H))).squeeze(-1)   # [B,S]\n",
        "        a = torch.softmax(s, dim=-1)\n",
        "        ctx = torch.einsum(\"bs,bsd->bd\", a, H)\n",
        "        return ctx, a\n",
        "\n",
        "class RationaleHead(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(dim, 1)\n",
        "    def forward(self, H):                         # [B*S,T,D]\n",
        "        return torch.sigmoid(self.lin(H).squeeze(-1))   # [B*S,T]\n",
        "\n",
        "def faithfulness_loss(attn_seg, rat_seg, margin=0.03):\n",
        "    pos = (rat_seg > 0.5).float()\n",
        "    if pos.sum() == 0:\n",
        "        return attn_seg.sum()*0.0\n",
        "    attn_pos = (attn_seg*pos).sum() / (pos.sum()+1e-8)\n",
        "    attn_neg = (attn_seg*(1-pos)).sum() / ((1-pos).sum()+1e-8)\n",
        "    return torch.relu(margin - (attn_pos - attn_neg))\n",
        "\n",
        "class LexFaithHierBERT(nn.Module):\n",
        "    \"\"\"Legal Faithfulness-Aware Hierarchical BERT\"\"\"\n",
        "    def __init__(self, name=\"bert-base-uncased\", num_classes=1, dropout=0.1, margin=0.03):\n",
        "        super().__init__()\n",
        "        self.segment_encoder = AutoModel.from_pretrained(name)\n",
        "        d = self.segment_encoder.config.hidden_size\n",
        "        self.seg_attn   = AdditiveAttention(d, dropout)\n",
        "        self.rationale  = RationaleHead(d)\n",
        "        self.classifier = nn.Linear(d, num_classes)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.margin = margin\n",
        "    def forward(self, input_ids, attention_mask):    # [B,S,T]\n",
        "        B,S,T = input_ids.shape\n",
        "        x = input_ids.reshape(B*S, T)\n",
        "        m = attention_mask.reshape(B*S, T)\n",
        "        out = self.segment_encoder(input_ids=x, attention_mask=m)\n",
        "        H = out.last_hidden_state                          # [B*S,T,D]\n",
        "        rat_tok = self.rationale(H)                        # [B*S,T]\n",
        "        cls = H[:,0,:].reshape(B, S, -1)                   # [B,S,D]\n",
        "        seg_ctx, a_seg = self.seg_attn(self.drop(cls))     # [B,D], [B,S]\n",
        "        logits = self.classifier(self.drop(seg_ctx))       # [B,C]\n",
        "        rat_seg = rat_tok.reshape(B, S, T).mean(dim=-1)    # [B,S]\n",
        "        f_loss = faithfulness_loss(a_seg, rat_seg, self.margin)\n",
        "        return logits, a_seg, rat_tok, f_loss\n",
        "\n",
        "# ===========================\n",
        "# Collate functions (hier/flat)\n",
        "# ===========================\n",
        "def collate_hier_texts(batch_texts, labels=None, cfg=CFG, tokenizer=TOK):\n",
        "    segs = []; masks=[]\n",
        "    pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n",
        "    for t in batch_texts:\n",
        "        s = text_to_segments(str(t), tokenizer, cfg)\n",
        "        a = pad_segments(s, cfg[\"max_segments\"], pad_id, cfg[\"segment_len\"])\n",
        "        segs.append(a)\n",
        "        masks.append((a != pad_id).astype(int))\n",
        "    X = torch.tensor(np.asarray(segs))\n",
        "    M = torch.tensor(np.asarray(masks))\n",
        "    Y = None\n",
        "    if labels is not None:\n",
        "        lab = np.asarray(labels)\n",
        "        if lab.ndim == 1: Y = torch.tensor(lab).float()\n",
        "        else: Y = torch.tensor(lab).float()\n",
        "    return X, M, Y\n",
        "\n",
        "def collate_flat_texts(batch_texts, labels=None, max_len=512, tokenizer=TOK):\n",
        "    # transformers-like encoding if available\n",
        "    try:\n",
        "        enc = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
        "        X = enc[\"input_ids\"]; M = enc[\"attention_mask\"]\n",
        "    except Exception:\n",
        "        # fallback: pad to same length\n",
        "        ids = [tokenizer(t)[\"input_ids\"] for t in batch_texts]\n",
        "        L = min(max_len, max(len(x) for x in ids))\n",
        "        arr = []\n",
        "        for x in ids:\n",
        "            xx = x[:L] + [getattr(tokenizer, \"pad_token_id\", 0)]*(L-len(x))\n",
        "            arr.append(xx)\n",
        "        X = torch.tensor(np.asarray(arr)); M = (X != getattr(tokenizer, \"pad_token_id\", 0)).long()\n",
        "    Y = None\n",
        "    if labels is not None:\n",
        "        lab = np.asarray(labels)\n",
        "        if lab.ndim == 1: Y = torch.tensor(lab).float()\n",
        "        else: Y = torch.tensor(lab).float()\n",
        "    return X, M, Y\n",
        "\n",
        "# ==========================\n",
        "# Training loops (concise)\n",
        "# ==========================\n",
        "def train_taskA_proposed(train_df, val_df, cfg=CFG):\n",
        "    device = \"cuda\" if (HAS_TORCH and torch.cuda.is_available()) else \"cpu\"\n",
        "    if not (HAS_TORCH and TRANS_OK):\n",
        "        raise RuntimeError(\"Transformers not available; cannot run proposed model.\")\n",
        "    model = LexFaithHierBERT(num_classes=1, dropout=cfg[\"dropout\"]).to(device)\n",
        "    opt = torch.optim.AdamW([\n",
        "        {\"params\": model.segment_encoder.parameters(), \"lr\": cfg[\"lr_encoder\"]},\n",
        "        {\"params\": list(model.seg_attn.parameters()) + list(model.rationale.parameters()) + list(model.classifier.parameters()), \"lr\": cfg[\"lr_head\"]}\n",
        "    ], weight_decay=0.01)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    best, wait = -1.0, 0\n",
        "    for ep in range(cfg[\"epochs_A\"]):\n",
        "        model.train()\n",
        "        # mini-batches\n",
        "        idx = np.arange(len(train_df)); np.random.shuffle(idx)\n",
        "        for i in range(0, len(idx), cfg[\"batch_size\"]):\n",
        "            j = idx[i:i+cfg[\"batch_size\"]]\n",
        "            bt = train_df.iloc[j]\n",
        "            X, M, Y = collate_hier_texts(bt[\"text\"].tolist(), bt[\"label\"].tolist(), cfg, TOK)\n",
        "            X,M,Y = X.to(device), M.to(device), Y.to(device)\n",
        "            logits, a_seg, rat_tok, f_loss = model(X,M)\n",
        "            loss = crit(logits.squeeze(-1), Y) + 0.1*f_loss\n",
        "            opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
        "        # validate\n",
        "        model.eval(); all_prob=[]; all_y=[]\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(val_df), cfg[\"batch_size\"]):\n",
        "                bt = val_df.iloc[i:i+cfg[\"batch_size\"]]\n",
        "                X,M,_ = collate_hier_texts(bt[\"text\"].tolist(), None, cfg, TOK)\n",
        "                X,M = X.to(device), M.to(device)\n",
        "                logits,_,_,_ = model(X,M)\n",
        "                p = torch.sigmoid(logits).squeeze(-1).cpu().numpy().tolist()\n",
        "                all_prob += p; all_y += bt[\"label\"].tolist()\n",
        "        res = metrics_taskA(np.array(all_y), np.array(all_prob))\n",
        "        print(f\"[TaskA][Ep {ep+1}] ACC={res['accuracy']:.3f} F1={res['f1']:.3f} AUC={res['roc_auc']:.3f}\")\n",
        "        if res[\"f1\"] > best:\n",
        "            best = res[\"f1\"]; wait = 0\n",
        "            best_metrics = res\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= cfg[\"early_stop_patience\"]: break\n",
        "    return best_metrics\n",
        "\n",
        "def train_taskB_proposed(train_df, val_df, cfg=CFG):\n",
        "    device = \"cuda\" if (HAS_TORCH and torch.cuda.is_available()) else \"cpu\"\n",
        "    if not (HAS_TORCH and TRANS_OK):\n",
        "        raise RuntimeError(\"Transformers not available; cannot run proposed model.\")\n",
        "    model = LexFaithHierBERT(num_classes=cfg[\"num_labels\"], dropout=cfg[\"dropout\"]).to(device)\n",
        "    opt = torch.optim.AdamW([\n",
        "        {\"params\": model.segment_encoder.parameters(), \"lr\": cfg[\"lr_encoder\"]},\n",
        "        {\"params\": list(model.seg_attn.parameters()) + list(model.rationale.parameters()) + list(model.classifier.parameters()), \"lr\": cfg[\"lr_head\"]}\n",
        "    ], weight_decay=0.01)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    best, wait = -1.0, 0\n",
        "    label_cols = [f\"y{i}\" for i in range(cfg[\"num_labels\"])]\n",
        "    for ep in range(cfg[\"epochs_B\"]):\n",
        "        model.train()\n",
        "        idx = np.arange(len(train_df)); np.random.shuffle(idx)\n",
        "        for i in range(0, len(idx), cfg[\"batch_size\"]):\n",
        "            j = idx[i:i+cfg[\"batch_size\"]]\n",
        "            bt = train_df.iloc[j]\n",
        "            Y = bt[label_cols].values\n",
        "            X,M,Y = collate_hier_texts(bt[\"text\"].tolist(), Y, cfg, TOK)\n",
        "            X,M,Y = X.to(device), M.to(device), Y.to(device)\n",
        "            logits, a_seg, rat_tok, f_loss = model(X,M)\n",
        "            loss = crit(logits, Y) + 0.1*f_loss\n",
        "            opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
        "        # validate\n",
        "        model.eval(); probs=[]; Yt=[]\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(val_df), cfg[\"batch_size\"]):\n",
        "                bt = val_df.iloc[i:i+cfg[\"batch_size\"]]\n",
        "                X,M,_ = collate_hier_texts(bt[\"text\"].tolist(), None, cfg, TOK)\n",
        "                X,M = X.to(device), M.to(device)\n",
        "                logits,_,_,_ = model(X,M)\n",
        "                p = torch.sigmoid(logits).cpu().numpy()\n",
        "                probs.append(p); Yt.append(bt[label_cols].values)\n",
        "        P = np.vstack(probs); YT = np.vstack(Yt)\n",
        "        res = metrics_taskB(YT, P, thr=0.5)\n",
        "        print(f\"[TaskB][Ep {ep+1}] microF1={res['micro_f1']:.3f} macroF1={res['macro_f1']:.3f} H={res['hamming_loss']:.3f}\")\n",
        "        if res[\"micro_f1\"] > best:\n",
        "            best = res[\"micro_f1\"]; wait = 0; best_metrics = res\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= cfg[\"early_stop_patience\"]: break\n",
        "    return best_metrics\n",
        "\n",
        "# =====================\n",
        "# Simple XAI interfaces\n",
        "# =====================\n",
        "def attention_tokens_preview(text):\n",
        "    # demo: uniform random weights (replace with model attentions)\n",
        "    toks = text.split()\n",
        "    w = np.random.rand(len(toks))\n",
        "    return list(zip(toks, w/w.max()))\n",
        "\n",
        "def lime_stub(text):\n",
        "    # put your LIME pipeline here (predict_fn wrapper around model)\n",
        "    return {\"explanation\": \"LIME scores (stub for demo)\"}\n",
        "\n",
        "def shap_stub(texts):\n",
        "    # put your SHAP pipeline here (Explainer over text)\n",
        "    return {\"explanation\": \"SHAP values (stub for demo)\"}\n",
        "\n",
        "# ========================\n",
        "# Statistical test helpers\n",
        "# ========================\n",
        "from scipy.stats import ttest_rel, f_oneway, chi2, norm\n",
        "\n",
        "def paired_t(values_a, values_b):\n",
        "    t, p = ttest_rel(values_a, values_b, alternative=\"greater\")\n",
        "    return float(t), float(p)\n",
        "\n",
        "def anova_all(*args):\n",
        "    f, p = f_oneway(*args)\n",
        "    return float(f), float(p)\n",
        "\n",
        "def z_test_acc(acc_a, acc_b, n_a, n_b):\n",
        "    p = (acc_a*n_a + acc_b*n_b) / (n_a+n_b)\n",
        "    se = math.sqrt(p*(1-p)*(1/n_a + 1/n_b))\n",
        "    z  = (acc_a-acc_b)/(se+1e-12)\n",
        "    pval = 2*(1-norm.cdf(abs(z)))\n",
        "    return float(z), float(pval)\n",
        "\n",
        "def chi_square_from_counts(tp, fp, fn, tn):\n",
        "    obs = np.array([tp, fp, fn, tn], dtype=float)\n",
        "    exp = np.ones_like(obs)*obs.mean()\n",
        "    stat = ((obs-exp)**2/(exp+1e-12)).sum()\n",
        "    p = 1-chi2.cdf(stat, df=len(obs)-1)\n",
        "    return float(stat), float(p)\n",
        "\n",
        "# =========================\n",
        "# Demo runner (edit or drop)\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    # -----------------------------\n",
        "    # FAKE DATA (replace with real)\n",
        "    # -----------------------------\n",
        "    # Task A\n",
        "    n_tr, n_va = 80, 20\n",
        "    dfA_tr = pd.DataFrame({\n",
        "        \"text\": [\"The applicant alleges unlawful detention and lack of counsel.\" if i%2==0 else\n",
        "                 \"The state argues due process was followed with timely review.\" for i in range(n_tr)],\n",
        "        \"label\": [1 if i%2==0 else 0 for i in range(n_tr)]\n",
        "    })\n",
        "    dfA_va = pd.DataFrame({\n",
        "        \"text\": [\"Prolonged detention without cause; judicial oversight absent.\" if i%2==0 else\n",
        "                 \"Evidence indicates no breach and adequate procedural safeguards.\" for i in range(n_va)],\n",
        "        \"label\": [1 if i%2==0 else 0 for i in range(n_va)]\n",
        "    })\n",
        "\n",
        "    # Task B (10 labels y0..y9)\n",
        "    n_trB, n_vaB = 60, 20\n",
        "    def rand_multi_lab(n, L=10):\n",
        "        Y = np.zeros((n,L), dtype=int)\n",
        "        for i in range(n):\n",
        "            k = np.random.randint(1,4)             # 1-3 labels active\n",
        "            idx = np.random.choice(L, k, replace=False)\n",
        "            Y[i, idx] = 1\n",
        "        return Y\n",
        "    Ytr = rand_multi_lab(n_trB, CFG[\"num_labels\"])\n",
        "    Yva = rand_multi_lab(n_vaB, CFG[\"num_labels\"])\n",
        "    dfB_tr = pd.DataFrame({\"text\":[f\"Case {i}: facts and legal analysis on multiple articles.\" for i in range(n_trB)]})\n",
        "    for j in range(CFG[\"num_labels\"]): dfB_tr[f\"y{j}\"]=Ytr[:,j]\n",
        "    dfB_va = pd.DataFrame({\"text\":[f\"Validation case {i}: complex facts for articles.\" for i in range(n_vaB)]})\n",
        "    for j in range(CFG[\"num_labels\"]): dfB_va[f\"y{j}\"]=Yva[:,j]\n",
        "\n",
        "    # -----------------------------\n",
        "    # Baseline example (Task A)\n",
        "    # -----------------------------\n",
        "    bowA = run_bow_logreg_taskA(dfA_tr, dfA_va)\n",
        "    print(\"[BoW+LogReg][TaskA]\", bowA)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Proposed (Task A & Task B)\n",
        "    # -----------------------------\n",
        "    if HAS_TORCH and TRANS_OK:\n",
        "        bestA = train_taskA_proposed(dfA_tr, dfA_va, CFG)\n",
        "        print(\"[Proposed LexFaith-HierBERT][TaskA]\", bestA)\n",
        "\n",
        "        bestB = train_taskB_proposed(dfB_tr, dfB_va, CFG)\n",
        "        print(\"[Proposed LexFaith-HierBERT][TaskB]\", bestB)\n",
        "    else:\n",
        "        print(\"Transformers not available; run with BiLSTM or enable transformers to train proposed model.\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # XAI quick preview (token-level)\n",
        "    # -----------------------------\n",
        "    xai_demo = attention_tokens_preview(\"The applicant was unlawfully detained and denied access to counsel.\")\n",
        "    print(\"Attention preview:\", xai_demo[:8], \"...\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Statistical tests demo\n",
        "    # (use arrays from repeated runs/folds; here random placeholders)\n",
        "    # -----------------------------\n",
        "    rng = np.random.default_rng(7)\n",
        "    prop_scores = rng.normal(0.88, 0.01, 5)  # pretend 5-fold accuracy for Task A\n",
        "    bow_scores  = rng.normal(0.75, 0.02, 5)\n",
        "\n",
        "    t, p = paired_t(prop_scores, bow_scores)\n",
        "    f, pa = anova_all(prop_scores, bow_scores)\n",
        "    z, pz = z_test_acc(prop_scores.mean(), bow_scores.mean(), 2200, 2200)  # example test sizes\n",
        "    chi, pc = chi_square_from_counts(1500, 200, 250, 250)                   # dummy counts\n",
        "\n",
        "    print(f\"[Stats][TaskA] t={t:.2f}, p={p:.4f}; ANOVA p={pa:.4f}; z={z:.2f}, p={pz:.4f}; chi2 p={pc:.4f}\")\n"
      ]
    }
  ]
}